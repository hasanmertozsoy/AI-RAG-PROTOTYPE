import os
import tempfile
import streamlit as st
from typing import List, Tuple, Optional
from dotenv import load_dotenv
from google import genai
from google.genai import types as genai_types
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.embeddings import Embeddings
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader

load_dotenv()
COLLECTION_NAME = "company_docs"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
DEFAULT_RETRIEVAL_K = 5
TEMPERATURE = 0.0

FILE_ICONS = {
    "pdf": "ğŸ“„",
    "docx": "ğŸ“",
    "txt": "ğŸ“ƒ",
    "md": "ğŸ—’ï¸",
}

st.set_page_config(
    page_title="Åirket Ä°Ã§i AI Asistan",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Fraunces:ital,wght@0,300;0,400;0,600;1,300&family=DM+Mono:wght@400;500&display=swap');
:root { --ink: #1a1714; --paper: #f5f0e8; --cream: #ede8dc; --rust: #c45c2e; --sage: #4a6741; --dust: #9b9589; --warm-white: #faf7f2; }
html, body, [class*="css"] { font-family: 'Fraunces', Georgia, serif; background-color: var(--paper); color: var(--ink); }
.stApp { background-color: var(--paper); }
section[data-testid="stSidebar"] { background-color: var(--ink); color: var(--warm-white); }
section[data-testid="stSidebar"] h1, section[data-testid="stSidebar"] h2, section[data-testid="stSidebar"] p, section[data-testid="stSidebar"] span, section[data-testid="stSidebar"] label, section[data-testid="stSidebar"] small { color: var(--warm-white) !important; }
section[data-testid="stSidebar"] [data-testid="stFileUploadDropzone"] { background-color: var(--ink) !important; border: 1px dashed rgba(245, 240, 232, 0.4) !important; }
section[data-testid="stSidebar"] [data-testid="stFileUploadDropzone"] div, section[data-testid="stSidebar"] [data-testid="stFileUploadDropzone"] small { color: var(--warm-white) !important; opacity: 1 !important; font-weight: 500 !important; }
section[data-testid="stSidebar"] [data-testid="stFileUploader"] button { background-color: var(--warm-white) !important; border: 1px solid var(--ink) !important; }
section[data-testid="stSidebar"] [data-testid="stFileUploader"] button, section[data-testid="stSidebar"] [data-testid="stFileUploader"] button * { color: var(--ink) !important; font-weight: 600 !important; font-family: 'DM Mono', monospace; }
section[data-testid="stSidebar"] .stButton > button { background-color: var(--warm-white) !important; border: 1px solid var(--ink) !important; transition: all 0.2s ease-in-out; }
section[data-testid="stSidebar"] .stButton > button, section[data-testid="stSidebar"] .stButton > button * { color: var(--ink) !important; font-family: 'DM Mono', monospace; font-weight: 600; }
section[data-testid="stSidebar"] .stButton > button:hover { background-color: var(--ink) !important; border: 1px solid var(--warm-white) !important; }
section[data-testid="stSidebar"] .stButton > button:hover, section[data-testid="stSidebar"] .stButton > button:hover * { color: var(--warm-white) !important; }
.stChatInput > div { border: 2px solid var(--ink) !important; background-color: var(--warm-white) !important; }
.stChatInput textarea { color: var(--ink) !important; }
.doc-card { background-color: rgba(245, 240, 232, 0.1); border: 1px solid rgba(245, 240, 232, 0.2); border-radius: 4px; padding: 0.5rem; margin: 0.3rem 0; font-family: 'DM Mono', monospace; font-size: 0.75rem; display: flex; align-items: center; gap: 0.5rem; }
.source-chip { background-color: var(--dust); border: 1px solid rgba(26, 23, 20, 0.2); padding: 0.1rem 0.4rem; border-radius: 12px; font-size: 0.7rem; font-family: 'DM Mono', monospace; margin-right: 4px; color: var(--ink) !important; display: inline-block; margin-bottom: 4px; }
</style>
""", unsafe_allow_html=True)

if "messages" not in st.session_state:
    st.session_state.messages = []
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "doc_stats" not in st.session_state:
    st.session_state.doc_stats = {"count": 0, "names": []}

class GeminiEmbeddings(Embeddings):
    def __init__(self, api_key: str):
        self._client = genai.Client(api_key=api_key)

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = self._client.models.embed_content(
            model="gemini-embedding-001",
            contents=texts,
            config=genai_types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
        )
        return [e.values for e in result.embeddings]

    def embed_query(self, text: str) -> List[float]:
        result = self._client.models.embed_content(
            model="gemini-embedding-001",
            contents=text,
            config=genai_types.EmbedContentConfig(task_type="RETRIEVAL_QUERY"),
        )
        return result.embeddings[0].values

def get_api_key():
    return os.environ.get("GEMINI_API_KEY")

def process_documents(uploaded_files):
    api_key = get_api_key()
    if not api_key:
        st.error("API AnahtarÄ± bulunamadÄ± (.env kontrol edin).")
        return None, 0, []

    loaders = {
        "pdf": PyPDFLoader,
        "docx": Docx2txtLoader,
        "txt": lambda p: TextLoader(p, encoding="utf-8"),
        "md": lambda p: TextLoader(p, encoding="utf-8"),
    }

    all_chunks = []
    processed_names = []
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    status_container = st.status("DokÃ¼manlar iÅŸleniyor...", expanded=True)
    
    for file in uploaded_files:
        ext = file.name.rsplit(".", 1)[-1].lower()
        if ext not in loaders:
            continue
            
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{ext}") as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name
        
        try:
            loader = loaders[ext](tmp_path)
            docs = loader.load()
            for doc in docs:
                doc.metadata["source"] = file.name
            
            file_chunks = splitter.split_documents(docs)
            all_chunks.extend(file_chunks)
            processed_names.append(file.name)
            status_container.write(f" {file.name} ({len(file_chunks)} parÃ§a)")
        except Exception as e:
            status_container.write(f" {file.name} hatasÄ±: {e}")
        finally:
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

    if not all_chunks:
        status_container.update(label="Ä°ÅŸlenecek metin bulunamadÄ± veya hata oluÅŸtu.", state="error")
        return None, 0, []

    status_container.write("VektÃ¶r veritabanÄ± oluÅŸturuluyor...")
    
    try:
        embeddings = GeminiEmbeddings(api_key)
        
        # PERSISTENCE (KalÄ±cÄ±lÄ±k) engellemek ve temiz bir baÅŸlangÄ±Ã§ yapmak iÃ§in:
        vectorstore = Chroma(
            collection_name=COLLECTION_NAME,
            embedding_function=embeddings,
            # persist_directory=None # EÄŸer diskte tutuyorsanÄ±z burayÄ± belirtmelisiniz
        )
        
        # Mevcut koleksiyonu tamamen sil
        try:
            vectorstore.delete_collection()
        except:
            pass

        # Åimdi temizlenmiÅŸ koleksiyona yeni dokÃ¼manlarÄ± ekle
        vectorstore = Chroma.from_documents(
            documents=all_chunks,
            embedding=embeddings,
            collection_name=COLLECTION_NAME
        )
        
        status_container.update(label="Ä°ÅŸlem BaÅŸarÄ±lÄ±!", state="complete", expanded=False)
        return vectorstore, len(all_chunks), processed_names
    except Exception as e:
        status_container.update(label=f"VektÃ¶r/Embedding hatasÄ±: {e}", state="error")
        return None, 0, []

def get_rag_response(query: str, vectorstore: Chroma, total_chunks: int):
    client = genai.Client(api_key=get_api_key())
    
    k = min(DEFAULT_RETRIEVAL_K, total_chunks) if total_chunks > 0 else 1
    retrieved_docs = vectorstore.similarity_search(query, k=k)

    if not retrieved_docs:
        return "DokÃ¼manlarda bu soruyla ilgili bilgi bulunamadÄ±.", []

    context_parts = []
    source_set = set()
    
    for d in retrieved_docs:
        src = d.metadata.get("source", "Bilinmeyen")
        page = d.metadata.get("page", None)
        ref = f"{src} (s.{int(page)+1})" if page is not None else src
        
        context_parts.append(f"--- Belge: {ref} ---\n{d.page_content}")
        source_set.add(ref)

    context_str = "\n\n".join(context_parts)
    sources = list(source_set)

    system_instruction = f"""Sen bir ÅŸirket iÃ§i dokÃ¼mantasyon asistanÄ±sÄ±n. GÃ¶revin, yalnÄ±zca sana verilen ÅŸirket dokÃ¼manlarÄ±na dayanarak sorularÄ± yanÄ±tlamaktÄ±r.

KURALLAR:
1. SADECE aÅŸaÄŸÄ±daki "BaÄŸlam" bÃ¶lÃ¼mÃ¼ndeki bilgileri kullan.
2. EÄŸer soru baÄŸlamdaki bilgilerle yanÄ±tlanamÄ±yorsa tam olarak ÅŸunu sÃ¶yle: "Bu bilgiye sahip deÄŸilim."
3. Asla tahmin yÃ¼rÃ¼tme, uydurma bilgi verme veya kendi genel bilginle yanÄ±t oluÅŸturma.
4. TÃ¼rkÃ§e soruda â†’ TÃ¼rkÃ§e yanÄ±tla; Ä°ngilizce soruda â†’ Ä°ngilizce yanÄ±tla.
5. YanÄ±tlarÄ±n aÃ§Ä±k, sade ve anlaÅŸÄ±lÄ±r olsun. Madde madde listeler kullanabilirsin.

BaÄŸlam:
{context_str}
"""
    try:
        resp = client.models.generate_content(
            model="gemini-flash-lite-latest",
            contents=query,
            config=genai_types.GenerateContentConfig(
                system_instruction=system_instruction,
                temperature=TEMPERATURE
            )
        )
        return resp.text, sources
    except Exception as e:
        return f"Model hatasÄ±: {str(e)}", []

with st.sidebar:
    st.title("DokÃ¼man YÃ¶netimi")
    uploaded_files = st.file_uploader(
        "Dosya YÃ¼kle", 
        type=list(FILE_ICONS.keys()), 
        accept_multiple_files=True
    )
    
    if st.button("DosyalarÄ± Ä°ÅŸle", use_container_width=True):
        if uploaded_files:
            st.session_state.vectorstore = None
            st.session_state.doc_stats = {"count": 0, "names": []}
            vs, count, names = process_documents(uploaded_files)
            if vs:
                st.session_state.vectorstore = vs
                st.session_state.doc_stats = {"count": count, "names": names}
                st.rerun()
        else:
            st.warning("LÃ¼tfen dosya seÃ§in.")

    st.divider()
    if st.session_state.doc_stats["names"]:
        st.caption(f"Ä°ndeks: {st.session_state.doc_stats['count']} parÃ§a")
        for name in st.session_state.doc_stats["names"]:
            ext = name.split(".")[-1]
            st.markdown(f'<div class="doc-card">{FILE_ICONS.get(ext, "ğŸ“„")} {name}</div>', unsafe_allow_html=True)
    else:
        st.info("HenÃ¼z dokÃ¼man iÅŸlenmedi.")

st.title("Åirket Ä°Ã§i AI Asistan")

if not st.session_state.vectorstore:
    st.info("BaÅŸlamak iÃ§in sol menÃ¼den dokÃ¼man yÃ¼kleyip 'Ä°ÅŸle' butonuna basÄ±nÄ±z.")
else:
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            # Sadece asistan mesajlarÄ±nda ve eÄŸer kaynak varsa gÃ¶ster
        if msg["role"] == "assistant" and msg.get("sources"):
            # KaynaklarÄ±n benzersiz olduÄŸundan emin olalÄ±m
            unique_sources = sorted(list(set(msg["sources"])))
            
            # Kaynak Ã§iplerini oluÅŸtur
            chips = "".join([
                f'<span class="source-chip">ğŸ“ {s}</span>' 
                for s in unique_sources
            ])
            
            # HTML ile temiz bir gÃ¶rÃ¼nÃ¼m saÄŸla
            st.markdown(
                f'<div style="margin-top: 8px; display: flex; flex-wrap: wrap; gap: 4px;">{chips}</div>', 
                unsafe_allow_html=True
            )

    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("DokÃ¼manlar taranÄ±yor..."):
                response_text, source_list = get_rag_response(
                    prompt, 
                    st.session_state.vectorstore, 
                    st.session_state.doc_stats["count"]
                )
                
                st.markdown(response_text)
                if source_list:
                    chips = "".join([f'<span class="source-chip">ğŸ“ {s}</span>' for s in source_list])
                    st.markdown(f"<br>{chips}", unsafe_allow_html=True)
        
        st.session_state.messages.append({
            "role": "assistant", 
            "content": response_text, 
            "sources": source_list
        })